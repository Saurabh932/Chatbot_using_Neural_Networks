{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a18add",
   "metadata": {},
   "source": [
    "## Contextual Chatbot:\n",
    "It is a type of conversational artificial intelligence that is designed to understand the context of the conversation and respond accordingly. The chatbot uses the context of the conversation to determine the user's intent and generate an appropriate response.\n",
    "\n",
    "\n",
    "### Tflearn\n",
    "TFLearn can be defined as a modular and transparent deep learning aspect used in TensorFlow framework. \n",
    "The main motive of TFLearn is to provide a higher level API to TensorFlow for facilitating and showing up new experiments.\n",
    "\n",
    "\n",
    "### Why Tflearn over Keras ?\n",
    "    1. With Tflearn we can use arrays directly. On other hand, in keras it needs numpy arrays.\n",
    "    2. Tflearn API is more closer to Tensorflow code as compared to keras.\n",
    "    3. Tflearn provides better performance than keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f46df",
   "metadata": {},
   "source": [
    "### Intent\n",
    "intent refers to the goal the customer has in mind when typing in a question or comment.\n",
    "Intent are very important component of the chatbot platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9fda68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labraries needed for NLP:\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer   # It convert any word to its root word\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e07516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labraries need for Tensorflow processing:\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7715ce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'],\n",
       "   'responses': ['Hello, thanks for visiting',\n",
       "    'Good to see you again',\n",
       "    'Hi there, how can I help?'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'goodbye',\n",
       "   'patterns': ['Bye', 'See you later', 'Goodbye'],\n",
       "   'responses': ['See you later, thanks for visiting',\n",
       "    'Have a nice day',\n",
       "    'Bye! Come back again soon.']},\n",
       "  {'tag': 'thanks',\n",
       "   'patterns': ['Thanks', 'Thank you', \"That's helpful\"],\n",
       "   'responses': ['Happy to help!', 'Any time!', 'My pleasure']},\n",
       "  {'tag': 'hours',\n",
       "   'patterns': ['What hours are you open?',\n",
       "    'What are your hours?',\n",
       "    'When are you open?'],\n",
       "   'responses': [\"We're open every day 9am-9pm\",\n",
       "    'Our hours are 9am-9pm every day']},\n",
       "  {'tag': 'location',\n",
       "   'patterns': ['What is your location?',\n",
       "    'Where are you located?',\n",
       "    'What is your address?',\n",
       "    'Where is your restaurant situated?'],\n",
       "   'responses': ['We are on the intersection of London Alley and Bridge Avenue.',\n",
       "    'We are situated at the intersection of London Alley and Bridge Avenue',\n",
       "    'Our Address is: 1000 Bridge Avenue, London EC3N 4AJ, UK']},\n",
       "  {'tag': 'payments',\n",
       "   'patterns': ['Do you take credit cards?',\n",
       "    'Do you accept Mastercard?',\n",
       "    'Are you cash only?'],\n",
       "   'responses': ['We accept VISA, Mastercard and AMEX',\n",
       "    'We accept most major credit cards']},\n",
       "  {'tag': 'todaysmenu',\n",
       "   'patterns': ['What is your menu for today?',\n",
       "    'What are you serving today?',\n",
       "    \"What is today's special?\"],\n",
       "   'responses': [\"Today's special is Chicken Tikka\",\n",
       "    'Our speciality for today is Chicken Tikka']},\n",
       "  {'tag': 'deliveryoption',\n",
       "   'patterns': ['Do you provide home delivery?',\n",
       "    'Do you deliver the food?',\n",
       "    'What are the home delivery options?'],\n",
       "   'responses': ['Yes, we provide home delivery through UBER Eats and Zomato?',\n",
       "    'We have home delivery options through UBER Eats and Zomato'],\n",
       "   'context_set': 'food'},\n",
       "  {'tag': 'menu',\n",
       "   'patterns': ['What is your Menu?',\n",
       "    'What are the main course options?',\n",
       "    'Can you tell me the most delicious dish from the menu?',\n",
       "    \"What is the today's special?\"],\n",
       "   'responses': ['You can visit www.mymenu.com for menu options',\n",
       "    'You can check out the food menu at www.mymenu.com',\n",
       "    'You can check various delicacies given in the food menu at www.mymenu.com'],\n",
       "   'context_filter': 'food'}]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "    \n",
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3faae77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore = [\"?\", \"!\", \"$\"]\n",
    "\n",
    "for intent in intents['intents']:  # loop through each sentence in intent's pattern\n",
    "    for pattern in intent['patterns']:\n",
    "        w = nltk.word_tokenize(pattern)  # tokenizing each and every word in the sentence\n",
    "        words.extend(w)\n",
    "        documents.append((w, intent['tag']))  # Storing values with their keys(tags) in the list of tuple\n",
    "        \n",
    "        if intent['tag'] not in classes:  # storing only keys(tags) in this.\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e90f5ef0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words is the list of pattern key in the dataset: \n",
      "['Hi', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?', 'What', 'are', 'you', 'serving', 'today', '?', 'What', 'is', 'today', \"'s\", 'special', '?', 'Do', 'you', 'provide', 'home', 'delivery', '?', 'Do', 'you', 'deliver', 'the', 'food', '?', 'What', 'are', 'the', 'home', 'delivery', 'options', '?', 'What', 'is', 'your', 'Menu', '?', 'What', 'are', 'the', 'main', 'course', 'options', '?', 'Can', 'you', 'tell', 'me', 'the', 'most', 'delicious', 'dish', 'from', 'the', 'menu', '?', 'What', 'is', 'the', 'today', \"'s\", 'special', '?'] \n",
      "\n",
      "Classes is the tags: \n",
      "['greeting', 'goodbye', 'thanks', 'hours', 'location', 'payments', 'todaysmenu', 'deliveryoption', 'menu'] \n",
      "\n",
      "Documents is the list of list which stores values and keys: \n",
      "[(['Hi'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['What', 'hours', 'are', 'you', 'open', '?'], 'hours'), (['What', 'are', 'your', 'hours', '?'], 'hours'), (['When', 'are', 'you', 'open', '?'], 'hours'), (['What', 'is', 'your', 'location', '?'], 'location'), (['Where', 'are', 'you', 'located', '?'], 'location'), (['What', 'is', 'your', 'address', '?'], 'location'), (['Where', 'is', 'your', 'restaurant', 'situated', '?'], 'location'), (['Do', 'you', 'take', 'credit', 'cards', '?'], 'payments'), (['Do', 'you', 'accept', 'Mastercard', '?'], 'payments'), (['Are', 'you', 'cash', 'only', '?'], 'payments'), (['What', 'is', 'your', 'menu', 'for', 'today', '?'], 'todaysmenu'), (['What', 'are', 'you', 'serving', 'today', '?'], 'todaysmenu'), (['What', 'is', 'today', \"'s\", 'special', '?'], 'todaysmenu'), (['Do', 'you', 'provide', 'home', 'delivery', '?'], 'deliveryoption'), (['Do', 'you', 'deliver', 'the', 'food', '?'], 'deliveryoption'), (['What', 'are', 'the', 'home', 'delivery', 'options', '?'], 'deliveryoption'), (['What', 'is', 'your', 'Menu', '?'], 'menu'), (['What', 'are', 'the', 'main', 'course', 'options', '?'], 'menu'), (['Can', 'you', 'tell', 'me', 'the', 'most', 'delicious', 'dish', 'from', 'the', 'menu', '?'], 'menu'), (['What', 'is', 'the', 'today', \"'s\", 'special', '?'], 'menu')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Words is the list of pattern key in the dataset: \\n{} \\n\\nClasses is the tags: \\n{} \\n\\nDocuments is the list of list which stores values and keys: \\n{}\".format(words, classes, documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e03a73c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(documents[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b7d2dace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 documents\n",
      "9 classes ['deliveryoption', 'goodbye', 'greeting', 'hours', 'location', 'menu', 'payments', 'thanks', 'todaysmenu']\n",
      "57 unique stemmed words\t [\"'s\", 'acceiv', 'address', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'cours', 'credit', 'day', 'del', 'delicy', 'delivery', 'dish', 'do', 'food', 'for', 'from', 'good', 'goodby', 'hello', 'help', 'hi', 'hom', 'hour', 'how', 'is', 'lat', 'loc', 'main', 'mastercard', 'me', 'menu', 'most', 'on', 'op', 'opt', 'provid', 'resta', 'see', 'serv', 'situ', 'spec', 'tak', 'tel', 'thank', 'that', 'the', 'ther', 'today', 'what', 'when', 'wher', 'yo', 'you']\n"
     ]
    }
   ],
   "source": [
    "# Perform stemming and lower each word as well as remove duplicates\n",
    "\n",
    "words = [stemmer.stem(str(w).lower()) for w in words if w not in ignore]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicate classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\\t\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58c962",
   "metadata": {},
   "source": [
    "### Creating the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9b64dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_11992\\742703838.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "# Creating an empty array for output\n",
    "op_empty = [0]*len(classes)\n",
    "\n",
    "# Creating training set, bags of words for each sentence\n",
    "for doc in documents:\n",
    "    bag = []   # Initalizing bag of words\n",
    "    pattern_words = doc[0]  # List of tokenizised words for the pattern\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]  # Stemming each word\n",
    "    \n",
    "    for w in words:  # creating bag of words array\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    # Output is '1' for current tag and '0' for rest of other tags\n",
    "    output_row = list(op_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "    \n",
    "# shuffling features and turning it into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5cd95e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the training into x and y train\n",
    "\n",
    "x_train = list(training[:,0])\n",
    "y_train = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dd872b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]] [[1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f9955",
   "metadata": {},
   "source": [
    "## Creating the Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aed6d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the underlying graph\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4831a55f",
   "metadata": {},
   "source": [
    "1. net = tflearn.input_data(shape=[None, len(x_train[0])]):  \n",
    "This line creates an input layer for the network and specifies the shape of the input data. The shape parameter takes a list of integers that define the shape of the input data. The first dimension, None, represents the number of samples in the data, which can be of any size. The second dimension, len(x_train[0]), represents the number of features in each sample, which is the number of words in the pattern.\n",
    " \n",
    "  \n",
    "2. net = tflearn.fully_connected(net, 10):  \n",
    "This line creates a fully connected layer in the network with 10 neurons. The first argument, net, is the input to the layer, which is the output from the previous layer. The second argument, 10, is the number of neurons in the layer.\n",
    "\n",
    "  \n",
    "3. net = tflearn.fully_connected(net, 10):   \n",
    "This line creates another fully connected layer with 10 neurons. The input to this layer is the output from the previous layer.\n",
    "\n",
    "  \n",
    "4. net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax'):  \n",
    "This line creates the final fully connected layer with len(y_train[0]) neurons. The activation function used in this layer is softmax, which is a common activation function used in the output layer of a neural network for classification tasks. The output of the softmax function is a probability distribution over the possible classes.\n",
    "\n",
    "  \n",
    "5. net = tflearn.regression(net): \n",
    "This line creates a regression model in TFLearn. The input to the model is the output from the previous layer. A regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db8bd3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural_network\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(x_train[0])])\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "net = tflearn.regression(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a699703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Defining model and setting up tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir = 'tflearn_logs')   # DNN =  Deep learning Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c82e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.12333\u001b[0m\u001b[0m | time: 0.017s\n",
      "| Adam | epoch: 1000 | loss: 0.12333 - acc: 0.9891 -- iter: 24/31\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.11586\u001b[0m\u001b[0m | time: 0.017s\n",
      "| Adam | epoch: 1000 | loss: 0.11586 - acc: 0.9902 -- iter: 31/31\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\acer\\Deep Learning\\Chat bot\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Starting training the model\n",
    "\n",
    "model.fit(x_train, y_train, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f41020",
   "metadata": {},
   "source": [
    "#### Pickle module:\n",
    "The pickle module in Python is used to serialize and deserialize Python objects, which means it can be used to save a trained machine learning or deep learning model to disk and load it back later to make predictions.\n",
    "\n",
    "In deep learning, training a model can be a time-consuming and computationally expensive task. After training the model, we may want to save it to avoid having to train it again from scratch each time we want to make predictions. The pickle module can be used to save the trained model as a binary file to disk, which can be loaded back into memory and used for making predictions.\n",
    "\n",
    "By using the pickle module, we can save the trained weights and architecture of the model, as well as any other necessary information such as the tokenizer or label encoder, to disk. This can be helpful when we want to deploy the model in a production environment or share it with other developers.\n",
    "\n",
    "Overall, the pickle module is a useful tool in deep learning for saving and loading trained models, making it a convenient way to use the models in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f8753dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({'words':words, 'classes':classes, 'x_train':x_train, 'y_train':y_train}, open('training_data', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4219ef",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614dbdd8",
   "metadata": {},
   "source": [
    "# Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "443ad624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e5d10646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading intent file again\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "59feaf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\acer\\Deep Learning\\Chat bot\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved model\n",
    "model.load('./model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "73f233e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_word = nltk.word_tokenize(sentence)    # Tokenizing the pattern\n",
    "    sentence_word = [stemmer.stem(word.lower()) for word in sentence_word]    # Stemming each word\n",
    "    return sentence_word\n",
    "\n",
    "# Returning bag of word(bow) array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    sentence_word = clean_up_sentence(sentence)  # tokenizing the pattern\n",
    "    \n",
    "    # Generating bag of words\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_word:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print('found in bag: %s'% w)\n",
    "    \n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f4131479",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_THRESHOLD = 0.30\n",
    "\n",
    "def classify(sentence):\n",
    "    results = model.predict([bow(sentence, words)])[0]  # Generating probabilites from the model\n",
    "    results = [[i,r] for i, r in enumerate(results) if r>ERROR_THRESHOLD]  # Filter out predictions below a Thershold value\n",
    "    results.sort(key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Sort by strength of probability\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "        \n",
    "    # Returning the tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, user_id='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    if results:  # If we have classification then find the matching intent tag\n",
    "        while results:  # Loop as long as there are matches to process\n",
    "            for i in intents['intents']:  # find the tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    return print(random.choice(i['responses']))\n",
    "            \n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "19223fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hours', 0.97457707)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('What are you hours of operation?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30e16f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our hours are 9am-9pm every day\n"
     ]
    }
   ],
   "source": [
    "response('What are you hours of operation?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b5d71c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our speciality for today is Chicken Tikka\n"
     ]
    }
   ],
   "source": [
    "response('What is menu for today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "14389936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We accept most major credit cards\n"
     ]
    }
   ],
   "source": [
    "#Some of other context free responses.\n",
    "response('Do you accept Credit Card?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1c327050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are on the intersection of London Alley and Bridge Avenue.\n"
     ]
    }
   ],
   "source": [
    "response('Where can we locate you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0c353288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any time!\n"
     ]
    }
   ],
   "source": [
    "response('That is helpful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08d0dd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have a nice day\n"
     ]
    }
   ],
   "source": [
    "response('Bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0da9b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding some context to the conversation i.e. Contexualization for altering question and intents etc.\n",
    "# create a data structure to hold user context\n",
    "context = {}\n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # set context for this intent if necessary\n",
    "                    if 'context_set' in i:\n",
    "                        if show_details: print ('context:', i['context_set'])\n",
    "                        context[userID] = i['context_set']\n",
    "\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in i or \\\n",
    "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('tag:', i['tag'])\n",
    "                        # a random response from the intent\n",
    "                        return print(random.choice(i['responses']))\n",
    "\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "06c3020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have home delivery options through UBER Eats and Zomato\n"
     ]
    }
   ],
   "source": [
    "response('Can you please let me know the delivery options?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "adbe3dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our speciality for today is Chicken Tikka\n"
     ]
    }
   ],
   "source": [
    "response('What is menu for today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5bca7249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': 'food'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bc42117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: \n",
      "tag: greeting\n",
      "Hello, thanks for visiting\n"
     ]
    }
   ],
   "source": [
    "response(\"Hi there!\", show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bed14596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's special is Chicken Tikka\n"
     ]
    }
   ],
   "source": [
    "response('What is menu for today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f173c3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
